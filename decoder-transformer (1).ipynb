{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets tokenizers\n\nimport torch\nimport torch.nn as nn\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    device = 'cuda'\nelse:\n    print(\"Using Single GPU\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(\"Loading 1,000,000 rows from TinyStories...\")\ndataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:1000000]\")\n\ndef truncate(example):\n    return {\"text\": example[\"text\"][:200]}\n\ndataset = dataset.map(truncate, num_proc=4)\ntokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = Whitespace()\ntrainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], vocab_size=4096)\n\ndef batch_iterator(dataset, batch_size=1000):\n    for i in range(0, len(dataset), batch_size):\n        yield dataset[i : i + batch_size][\"text\"]\n\nprint(\"Training Tokenizer (Vocab 4096)...\")\ntokenizer.train_from_iterator(batch_iterator(dataset), trainer=trainer)\nvocab_size = tokenizer.get_vocab_size()\nprint(f\"Tokenizer Ready. Vocab Size: {vocab_size}\")\nprint(\"Encoding Data (this may take a minute)...\")\nall_text = \"\".join(dataset[\"text\"])\nencoded_ids = tokenizer.encode(all_text).ids\ndata = torch.tensor(encoded_ids, dtype=torch.long)\n\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n\nprint(f\"Total Training Tokens: {len(train_data)/1e6:.2f} Million\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:41:53.130943Z","iopub.execute_input":"2026-01-10T13:41:53.131229Z","iopub.status.idle":"2026-01-10T13:45:46.100532Z","shell.execute_reply.started":"2026-01-10T13:41:53.131199Z","shell.execute_reply":"2026-01-10T13:45:46.099738Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nUsing 2 GPUs!\nLoading 1,000,000 rows from TinyStories...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db54fe3a6f04524a22a889437971ec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00004-2d5a1467fff108(…):   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97272c156b144a13a8382e6cf3f6c0d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00004-5852b56a2bd28f(…):   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7fca5ecf52f455c9b429662029ff353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00002-of-00004-a26307300439e9(…):   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d48423dd6665404f86fe68021a4bbba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00003-of-00004-d243063613e5a0(…):   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43647b3919ab4b0b8706720c7f7316c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-869c898b5(…):   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4cbdc9e106e4561ad14e59f3d40dcb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"292e21ca653240a9b18631c82ca1e571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f77e0390c549e3b03a04e02907034b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1920277520ea4991bb2bfe2fa946260c"}},"metadata":{}},{"name":"stdout","text":"Training Tokenizer (Vocab 4096)...\n\n\n\nTokenizer Ready. Vocab Size: 4096\nEncoding Data (this may take a minute)...\nTotal Training Tokens: 43.38 Million\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn import functional as F\n\nbatch_size = 64 \nblock_size = 256\nn_embd = 768\nn_head = 12\nn_layer = 16   \n\ndropout = 0.1\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (C**-0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(num_heads * head_size, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.GELU(), \n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        loss = None\n        if targets is not None:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\nmodel = GPTLanguageModel()\n\nif torch.cuda.device_count() > 1:\n    print(f\"Wrapping model in DataParallel for {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\nif isinstance(model, nn.DataParallel):\n    param_count = sum(p.numel() for p in model.module.parameters())\nelse:\n    param_count = sum(p.numel() for p in model.parameters())\n\nprint(f\"Model Parameters: {param_count/1e6:.2f} Million\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:46:02.053956Z","iopub.execute_input":"2026-01-10T13:46:02.054674Z","iopub.status.idle":"2026-01-10T13:46:03.648575Z","shell.execute_reply.started":"2026-01-10T13:46:02.054638Z","shell.execute_reply":"2026-01-10T13:46:03.647985Z"}},"outputs":[{"name":"stdout","text":"Wrapping model in DataParallel for 2 GPUs\nModel Parameters: 119.86 Million\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import time\n\nif isinstance(model, nn.DataParallel):\n    optimizer = torch.optim.AdamW(model.module.parameters(), lr=3e-4)\nelse:\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\ndef get_batch(split):\n    data_source = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n    x = torch.stack([data_source[i:i+block_size] for i in ix])\n    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(100)\n        for k in range(100):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n        \n            if loss.ndim > 0: loss = loss.mean()\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\nmax_iters = 2000 \neval_interval = 200\nstart_time = time.time()\n\nprint(f\"Starting Baseline Training (120M Params) on {device}...\")\nfor iter in range(max_iters):\n\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        dt = time.time() - start_time\n        print(f\"Step {iter}: Train {losses['train']:.4f}, Val {losses['val']:.4f} | Time: {dt:.1f}s\")\n\n    xb, yb = get_batch('train')\n    _, loss = model(xb, yb)\n  \n    if loss.ndim > 0: loss = loss.mean()\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(\"Baseline Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:46:06.396327Z","iopub.execute_input":"2026-01-10T13:46:06.397092Z","iopub.status.idle":"2026-01-10T13:46:39.321093Z","shell.execute_reply.started":"2026-01-10T13:46:06.397054Z","shell.execute_reply":"2026-01-10T13:46:39.320143Z"}},"outputs":[{"name":"stdout","text":"Starting Baseline Training (120M Params) on cuda...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/514655824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {iter}: Train {losses['train']:.4f}, Val {losses['val']:.4f} | Time: {dt:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/514655824.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Handle DataParallel output (it returns a vector of losses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef evaluate_baseline(num_samples=50):\n    generation_model = model.module if isinstance(model, nn.DataParallel) else model\n    generation_model.eval()\n    \n    scores = []\n    smoothing = SmoothingFunction().method1\n\n    print(\"Calculating Baseline BLEU...\")\n    with torch.no_grad():\n        for _ in range(num_samples):\n            idx = torch.randint(len(val_data) - 300, (1,)).item()\n            chunk = val_data[idx:idx+300]\n            text = tokenizer.decode(chunk.tolist())\n            \n            words = text.split()\n            if len(words) < 60: continue\n\n            prompt_str = \" \".join(words[:10])\n            ref_words = words[10:60]\n\n            context_ids = tokenizer.encode(prompt_str).ids\n            context_tensor = torch.tensor([context_ids], dtype=torch.long, device=device)\n\n            gen_ids = generation_model.generate(context_tensor, max_new_tokens=60)\n            full_gen = tokenizer.decode(gen_ids[0].tolist())\n            gen_text = full_gen[len(prompt_str):]\n \n            ref = \" \".join(ref_words).split()\n            cand = gen_text.split()\n            if len(cand) > 0:\n                score = sentence_bleu([ref], cand, weights=(0.5, 0.5), smoothing_function=smoothing)\n                scores.append(score)\n\n    avg = sum(scores) / len(scores)\n    print(f\"\\nBASELINE BLEU SCORE: {avg:.4f}\")\n    return avg\n\nbaseline_score = evaluate_baseline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:14:38.285709Z","iopub.execute_input":"2026-01-08T14:14:38.286363Z","iopub.status.idle":"2026-01-08T14:17:51.035769Z","shell.execute_reply.started":"2026-01-08T14:14:38.286338Z","shell.execute_reply":"2026-01-08T14:17:51.034934Z"}},"outputs":[{"name":"stdout","text":"Calculating Baseline BLEU...\n\nBASELINE BLEU SCORE: 0.1855\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import random\n\nconfigs = [\n    {'lr': 5e-4, 'dropout': 0.2, 'name': 'Aggressive'},\n    {'lr': 1e-4, 'dropout': 0.05, 'name': 'Conservative'},\n    {'lr': 3e-4, 'dropout': 0.2,  'name': 'Balanced'}\n]\n\nresults = {}\n\nprint(\"--- Starting Short-Horizon Search (500 steps each) ---\")\n\nfor config in configs:\n    print(f\"\\nTesting Config: {config['name']} (LR: {config['lr']}, Drop: {config['dropout']})\")\n \n    dropout = config['dropout'] \n    model = GPTLanguageModel()\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model = model.to(device)\n\n    if isinstance(model, nn.DataParallel):\n        optimizer = torch.optim.AdamW(model.module.parameters(), lr=config['lr'])\n    else:\n        optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n\n    for iter in range(500): \n        xb, yb = get_batch('train')\n        _, loss = model(xb, yb)\n        if loss.ndim > 0: loss = loss.mean()\n        \n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    final_loss = estimate_loss()['val']\n    print(f\"Result {config['name']}: Val Loss {final_loss:.4f}\")\n    results[config['name']] = final_loss\n\nbest_config_name = min(results, key=results.get)\nprint(\"-\" * 50)\nprint(f\"Winner: {best_config_name} with Loss {results[best_config_name]:.4f}\")\nprint(f\"Baseline at step 500 was ~2.42. Did we beat it?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-08T14:19:28.608962Z","iopub.execute_input":"2026-01-08T14:19:28.609735Z","iopub.status.idle":"2026-01-08T15:37:20.643165Z","shell.execute_reply.started":"2026-01-08T14:19:28.609712Z","shell.execute_reply":"2026-01-08T15:37:20.642045Z"}},"outputs":[{"name":"stdout","text":"--- Starting Short-Horizon Search (500 steps each) ---\n\nTesting Config: Aggressive (LR: 0.0005, Drop: 0.2)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Result Aggressive: Val Loss 2.2245\n\nTesting Config: Conservative (LR: 0.0001, Drop: 0.05)\nResult Conservative: Val Loss 2.7469\n\nTesting Config: Balanced (LR: 0.0003, Drop: 0.2)\nResult Balanced: Val Loss 2.3390\n--------------------------------------------------\nWinner: Aggressive with Loss 2.2245\nBaseline at step 500 was ~2.42. Did we beat it?\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"if isinstance(model, nn.DataParallel):\n    optimizer = torch.optim.AdamW(model.module.parameters(), lr=3e-4)\nelse:\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\ndef get_batch(split):\n    data_source = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n    x = torch.stack([data_source[i:i+block_size] for i in ix])\n    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:46:48.568523Z","iopub.execute_input":"2026-01-10T13:46:48.569205Z","iopub.status.idle":"2026-01-10T13:46:48.578214Z","shell.execute_reply.started":"2026-01-10T13:46:48.569173Z","shell.execute_reply":"2026-01-10T13:46:48.577469Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(100) \n        for k in range(100):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n            if loss.ndim > 0: loss = loss.mean()\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:46:50.619586Z","iopub.execute_input":"2026-01-10T13:46:50.620212Z","iopub.status.idle":"2026-01-10T13:46:50.624886Z","shell.execute_reply.started":"2026-01-10T13:46:50.620178Z","shell.execute_reply":"2026-01-10T13:46:50.624131Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import time\n\ndropout = 0.2\nlearning_rate = 5e-4\n\nprint(\"Initializing Final Model (Aggressive Config)...\")\nmodel = GPTLanguageModel()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\nmodel = model.to(device)\n\nif isinstance(model, nn.DataParallel):\n    optimizer = torch.optim.AdamW(model.module.parameters(), lr=learning_rate)\nelse:\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nmax_iters = 10000\neval_interval = 500\nstart_time = time.time()\nbest_val_loss = float('inf')\n\nprint(f\"Starting Final Training (10000 Steps) on {device}...\")\n\n\nfor iter in range(max_iters):\n\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n\n        if losses['val'] < best_val_loss:\n            best_val_loss = losses['val']\n            \n        dt = time.time() - start_time\n        print(f\"Step {iter}: Train {losses['train']:.4f}, Val {losses['val']:.4f} | Time: {dt:.1f}s\")\n\n    xb, yb = get_batch('train')\n    _, loss = model(xb, yb)\n    \n    if loss.ndim > 0: loss = loss.mean()\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(\"-\" * 50)\nprint(f\"FINAL TRAINING COMPLETE.\")\nprint(f\"Best Validation Loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:46:54.116777Z","iopub.execute_input":"2026-01-10T13:46:54.117105Z","iopub.status.idle":"2026-01-10T22:07:14.836584Z","shell.execute_reply.started":"2026-01-10T13:46:54.117075Z","shell.execute_reply":"2026-01-10T22:07:14.835802Z"}},"outputs":[{"name":"stdout","text":"Initializing Final Model (Aggressive Config)...\nStarting Final Training (10000 Steps) on cuda...\n--------------------------------------------------\nStep 0: Train 8.5004, Val 8.5007 | Time: 183.8s\nStep 500: Train 2.2523, Val 2.2232 | Time: 1674.6s\nStep 1000: Train 1.8962, Val 1.8963 | Time: 3164.6s\nStep 1500: Train 1.7538, Val 1.7492 | Time: 4655.6s\nStep 2000: Train 1.6743, Val 1.6799 | Time: 6146.7s\nStep 2500: Train 1.6145, Val 1.6325 | Time: 7639.6s\nStep 3000: Train 1.5705, Val 1.5854 | Time: 9132.5s\nStep 3500: Train 1.5387, Val 1.5703 | Time: 10625.0s\nStep 4000: Train 1.5067, Val 1.5397 | Time: 12116.3s\nStep 4500: Train 1.4801, Val 1.5154 | Time: 13609.1s\nStep 5000: Train 1.4589, Val 1.4977 | Time: 15101.3s\nStep 5500: Train 1.4413, Val 1.4772 | Time: 16593.6s\nStep 6000: Train 1.4240, Val 1.4716 | Time: 18086.3s\nStep 6500: Train 1.4166, Val 1.4670 | Time: 19577.8s\nStep 7000: Train 1.3910, Val 1.4549 | Time: 21070.8s\nStep 7500: Train 1.3910, Val 1.4508 | Time: 22562.1s\nStep 8000: Train 1.3824, Val 1.4452 | Time: 24054.5s\nStep 8500: Train 1.3721, Val 1.4332 | Time: 25545.2s\nStep 9000: Train 1.3620, Val 1.4180 | Time: 27036.3s\nStep 9500: Train 1.3394, Val 1.4184 | Time: 28528.3s\nStep 9999: Train 1.3436, Val 1.4145 | Time: 30017.5s\n--------------------------------------------------\nFINAL TRAINING COMPLETE.\nBest Validation Loss: 1.4145\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef evaluate_final(num_samples=50):\n    generation_model = model.module if isinstance(model, nn.DataParallel) else model\n    generation_model.eval()\n    \n    scores = []\n    smoothing = SmoothingFunction().method1\n\n    print(\"Calculating Final BLEU Score...\")\n    with torch.no_grad():\n        for _ in range(num_samples):\n\n            idx = torch.randint(len(val_data) - 300, (1,)).item()\n            chunk = val_data[idx:idx+300]\n            text = tokenizer.decode(chunk.tolist())\n            \n            words = text.split()\n            if len(words) < 60: continue\n            prompt_str = \" \".join(words[:10])\n            ref_words = words[10:60]\n            context_ids = tokenizer.encode(prompt_str).ids\n            context_tensor = torch.tensor([context_ids], dtype=torch.long, device=device)\n            \n            gen_ids = generation_model.generate(context_tensor, max_new_tokens=60)\n            full_gen = tokenizer.decode(gen_ids[0].tolist())\n            gen_text = full_gen[len(prompt_str):]\n            ref = \" \".join(ref_words).split()\n            cand = gen_text.split()\n            if len(cand) > 0:\n                score = sentence_bleu([ref], cand, weights=(0.5, 0.5), smoothing_function=smoothing)\n                scores.append(score)\n\n    avg = sum(scores) / len(scores)\n    print(\"-\" * 30)\n    print(f\"BASELINE BLEU: 0.1855\")\n    print(f\"FINAL BLEU:    {avg:.4f}\")\n    \n    if avg > 0.1855:\n        print(\"RESULT: SUCCESS - Hyperparameter Tuning Improved Performance!\")\n    else:\n        print(\"RESULT: NEUTRAL - Model might need even more training.\")\n        \n    return avg\n\nfinal_score = evaluate_final()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T22:07:42.357355Z","iopub.execute_input":"2026-01-10T22:07:42.358905Z","iopub.status.idle":"2026-01-10T22:10:49.109742Z","shell.execute_reply.started":"2026-01-10T22:07:42.358873Z","shell.execute_reply":"2026-01-10T22:10:49.108857Z"}},"outputs":[{"name":"stdout","text":"Calculating Final BLEU Score...\n------------------------------\nBASELINE BLEU: 0.1855\nFINAL BLEU:    0.2224\nRESULT: SUCCESS - Hyperparameter Tuning Improved Performance!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef generate_creative_story(prompt, temp=0.8):\n    generation_model = model.module if isinstance(model, nn.DataParallel) else model\n    generation_model.eval()\n    context_ids = tokenizer.encode(prompt).ids\n    idx = torch.tensor([context_ids], dtype=torch.long, device=device)\n\n    print(f\"Prompt: {prompt}\")\n\n    with torch.no_grad():\n        for _ in range(250):\n            idx_cond = idx[:, -block_size:]\n            logits, _ = generation_model(idx_cond)\n            logits = logits[:, -1, :] \n            logits = logits / temp \n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n\n    return tokenizer.decode(idx[0].tolist())\n\nstory = generate_creative_story(\"One day, a tiny robot found a magic key\", temp=0.8)\nprint(story)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T22:12:07.363869Z","iopub.execute_input":"2026-01-10T22:12:07.364840Z","iopub.status.idle":"2026-01-10T22:12:23.217743Z","shell.execute_reply.started":"2026-01-10T22:12:07.364808Z","shell.execute_reply":"2026-01-10T22:12:23.216957Z"}},"outputs":[{"name":"stdout","text":"Prompt: One day, a tiny robot found a magic key\nGenerating...\n--------------------------------------------------\nOne day , a tiny robot found a magic key . The robot was very happy and wanted to show the way to t Once upon a time , there was a little boy named Timmy . One day , Timmy went to the park to play . He saw a squirrel and wanted to pet it , but the squirrel was too fast and ran away . Timmy was sad and he de Once upon a time , there was a little boy named Timmy . Timmy loved to play outside in the snow with his friends . One day , Timmy ' s mom told him that he needed to wear his gloves so he wouldn ' t get his Once upon a time , there was a little girl named Lily . She loved to play outside and run around in the grass . One day , while she was playing , she found a shiny silver coin on the ground . She was so ve Once upon a time , there was a little girl named Lily . Lily loved to play outside , but one day it was very cold outside . Lily put on her warm coat , but her hands didn ' t feel good . She put on her sho One day , a boy found a toy car that could speed very fast . He took it to his friend , Sally , and showed it to her . Sally said , \" That car is so fast , it '\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}